import{_ as p}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,o as l,c as s,a as t,d as e,b as a,w as r,e as c}from"./app-dtuWjiEd.js";const h={},g=c('<h1 id="快速上手gpt-4o-免费使用指南-2024年6月最新" tabindex="-1"><a class="header-anchor" href="#快速上手gpt-4o-免费使用指南-2024年6月最新"><span>快速上手GPT-4o！免费使用指南（2024年6月最新）</span></a></h1><h2 id="gpt-4o简介" tabindex="-1"><a class="header-anchor" href="#gpt-4o简介"><span>GPT-4o简介</span></a></h2><p>北京时间 5 月 14 日，OpenAI 举办了春季发布会。此次活动中推出了新的旗舰模型“GPT-4o”！据 OpenAI 的首席技术官穆里·穆拉蒂（Muri Murati）所讲，GPT-4o 不但继承了 GPT-4 的强大智能，而且还进一步增强了文本、图像以及语音的处理能力，能给用户带来更为流畅、自然的交互体验。 比如说，像处理复杂的文本创作，GPT-4o 可能会表现得更加出色；在图像识别和理解方面，它或许能提供更精准的结果；而对于语音交互，也会让人感觉更加舒适和自然。</p><h2 id="gpt-4o的能力" tabindex="-1"><a class="header-anchor" href="#gpt-4o的能力"><span>GPT-4o的能力</span></a></h2><p>GPT-4o 具备在音频、视觉和文本领域进行实时推理的能力。其能够接纳文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合作为输出。对于音频输入，GPT-4o 最短响应时间为 232 毫秒，平均响应时间为 320 毫秒，此响应速度与人类在对话中的表现相近。</p><h3 id="文本能力" tabindex="-1"><a class="header-anchor" href="#文本能力"><span>文本能力</span></a></h3><p>在文本能力方面，GPT-4o 在 0 次 COT MMLU（常识问题）上创下了 88.7% 的高分纪录。所有相关评估均采用了我们全新的简单评估方式（在新窗口中打开）。此外，在传统的 5 次无 CoT MMLU 中，GPT-4o 同样取得了 87.2% 的新高成绩。（注：3400b（在新窗口中打开）仍处于训练阶段）</p><figure><img src="https://kangkpictures.oss-cn-beijing.aliyuncs.com/202406232155426.png" alt="" width="400px" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="音频能力" tabindex="-1"><a class="header-anchor" href="#音频能力"><span>音频能力</span></a></h3><p>GPT-4o 在语音翻译领域确立了新的领先优势，于 MLS 基准测试中表现优于 Whisper-v3 。</p><figure><img src="https://kangkpictures.oss-cn-beijing.aliyuncs.com/202406232202699.png" alt="" width="400px" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="视觉能力" tabindex="-1"><a class="header-anchor" href="#视觉能力"><span>视觉能力</span></a></h3><p>GPT-4o 在视觉感知基准测试中达成了最先进的性能水平，全面超越此前的模型。所有视觉评估次数均为 0 次，其中包括 MMMU、MathVista 和 ChartQA 的 0 次 CoT 评估。</p><h3 id="语音交互" tabindex="-1"><a class="header-anchor" href="#语音交互"><span>语音交互</span></a></h3><p>GPT-4o 在语音交互领域获取了重大突破。其运用了前沿技术，极大地提升了响应速度，令对话愈发流畅自然。于近期的发布会上，OpenAI 呈现了 GPT-4o 在语音对话中的性能，它能够近乎实时地应答问题，并借助文本转语音技术予以朗读，给予了一种沉浸式的交流感受。另外，GPT-4o 还能够对说话的语气加以调节，从夸张戏剧型到冰冷机械型，以契合不同的交流情境。令人欣喜的是，GPT-4o 还拥有唱歌的能力，增添了更多的趣味和娱乐元素。</p><h2 id="如何使用gpt-4o" tabindex="-1"><a class="header-anchor" href="#如何使用gpt-4o"><span>如何使用gpt-4o</span></a></h2><h3 id="环境准备" tabindex="-1"><a class="header-anchor" href="#环境准备"><span>环境准备</span></a></h3>',17),u=t("strong",null,"ke xue上网。",-1),d={href:"https://ikuuu.pw",target:"_blank",rel:"noopener noreferrer"},P=t("strong",null,"open AI账号",-1),T=t("h3",{id:"登录chatgpt",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#登录chatgpt"},[t("span",null,"登录chatgpt")])],-1),G=t("p",null,"在接下来的数周内，OpenAI 打算在 ChatGPT Plus 中推出带有 GPT-4o 的 Voice Mode 新版本。此版本将作为 ChatGPT Plus 的一个 alpha 版提供给 Plus 用户。此外，GPT-4o 还会通过 API 向开发者供应，作为文本和视觉模型。开发者能够借助 API 将 GPT-4o 集成至自身的应用程序中，而且 GPT-4o 在 API 里相较 GPT-4Tubo 更迅速、更经济，并有更高的速率限制。 至于 GPT-4o 的音频和视频功能，OpenAl 会在未来数周及数月持续构建技术基础设施、通过训练提升可用性并保障安全性，而后发布这些功能，并逐步向公众开放。",-1),m={href:"https://wildcard.com.cn/i/GPT521",target:"_blank",rel:"noopener noreferrer"};function _(f,k){const o=i("ExternalLinkIcon"),n=i("RouteLink");return l(),s("div",null,[g,t("ul",null,[t("li",null,[u,e(" (PS：如果还不会，推荐"),t("a",d,[e("https://ikuuu.pw"),a(o)]),e(", 有免费版，当然最好买个收费的，相对其他的也不贵)")]),t("li",null,[P,e("。 最好准备一个openAI账号，才能拥有对话记录功能。参考这篇教程免费获取: "),a(n,{to:"/tutorial_of_ChatGPT/how_to_register_openAI_ID.html"},{default:r(()=>[e("openAI账号获取")]),_:1}),e("。并登录自己的账号。")])]),T,t("p",null,[e("当下，GPT-4o 的文本和图像功能已在 ChatGPT 中推出，用户能于 ChatGPT 平台免费体验 GPT-4o 的相关功能，不过免费版存在使用次数限制，Plus 用户可享有 5 倍的调用额度（ps：如果没有openAI账号，可以参考这篇"),a(n,{to:"/tutorial_of_ChatGPT/GPT4_upgrade_tutorial.html"},{default:r(()=>[e("2分钟升级ChatGPT plus")]),_:1}),e("）。")]),G,t("p",null,[e("（ps： 如果不需要对话记录功能，可以选择wildcard随心用，价格是官网价五折："),t("a",m,[e("ChatGPT plus随心用"),a(o)]),e("）")])])}const y=p(h,[["render",_],["__file","GPT4o.html.vue"]]),A=JSON.parse('{"path":"/tutorial_of_ChatGPT/GPT4o.html","title":"快速上手GPT-4o！免费使用指南（2024年6月最新）","lang":"zh-CN","frontmatter":{"cover":"https://kangkpictures.oss-cn-beijing.aliyuncs.com/202406232141527.png","category":["教程","ChatGPT4","gpt4o"],"icon":"gears","description":"快速上手GPT-4o！免费使用指南（2024年6月最新） GPT-4o简介 北京时间 5 月 14 日，OpenAI 举办了春季发布会。此次活动中推出了新的旗舰模型“GPT-4o”！据 OpenAI 的首席技术官穆里·穆拉蒂（Muri Murati）所讲，GPT-4o 不但继承了 GPT-4 的强大智能，而且还进一步增强了文本、图像以及语音的处理能力，...","head":[["meta",{"property":"og:url","content":"https://kangk5g.github.io/chatgpt/tutorial_of_ChatGPT/GPT4o.html"}],["meta",{"property":"og:site_name","content":"ChatGPT中文指南"}],["meta",{"property":"og:title","content":"快速上手GPT-4o！免费使用指南（2024年6月最新）"}],["meta",{"property":"og:description","content":"快速上手GPT-4o！免费使用指南（2024年6月最新） GPT-4o简介 北京时间 5 月 14 日，OpenAI 举办了春季发布会。此次活动中推出了新的旗舰模型“GPT-4o”！据 OpenAI 的首席技术官穆里·穆拉蒂（Muri Murati）所讲，GPT-4o 不但继承了 GPT-4 的强大智能，而且还进一步增强了文本、图像以及语音的处理能力，..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://kangkpictures.oss-cn-beijing.aliyuncs.com/202406232141527.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://kangkpictures.oss-cn-beijing.aliyuncs.com/202406232141527.png"}],["meta",{"name":"twitter:image:alt","content":"快速上手GPT-4o！免费使用指南（2024年6月最新）"}],["meta",{"property":"article:author","content":"Kangk"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"快速上手GPT-4o！免费使用指南（2024年6月最新）\\",\\"image\\":[\\"https://kangkpictures.oss-cn-beijing.aliyuncs.com/202406232155426.png\\",\\"https://kangkpictures.oss-cn-beijing.aliyuncs.com/202406232202699.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kangk\\",\\"url\\":\\"\\"}]}"]]},"headers":[{"level":2,"title":"GPT-4o简介","slug":"gpt-4o简介","link":"#gpt-4o简介","children":[]},{"level":2,"title":"GPT-4o的能力","slug":"gpt-4o的能力","link":"#gpt-4o的能力","children":[{"level":3,"title":"文本能力","slug":"文本能力","link":"#文本能力","children":[]},{"level":3,"title":"音频能力","slug":"音频能力","link":"#音频能力","children":[]},{"level":3,"title":"视觉能力","slug":"视觉能力","link":"#视觉能力","children":[]},{"level":3,"title":"语音交互","slug":"语音交互","link":"#语音交互","children":[]}]},{"level":2,"title":"如何使用gpt-4o","slug":"如何使用gpt-4o","link":"#如何使用gpt-4o","children":[{"level":3,"title":"环境准备","slug":"环境准备","link":"#环境准备","children":[]},{"level":3,"title":"登录chatgpt","slug":"登录chatgpt","link":"#登录chatgpt","children":[]}]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":3.67,"words":1100},"filePathRelative":"tutorial_of_ChatGPT/GPT4o.md","excerpt":"\\n<h2>GPT-4o简介</h2>\\n<p>北京时间 5 月 14 日，OpenAI 举办了春季发布会。此次活动中推出了新的旗舰模型“GPT-4o”！据 OpenAI 的首席技术官穆里·穆拉蒂（Muri Murati）所讲，GPT-4o 不但继承了 GPT-4 的强大智能，而且还进一步增强了文本、图像以及语音的处理能力，能给用户带来更为流畅、自然的交互体验。 比如说，像处理复杂的文本创作，GPT-4o 可能会表现得更加出色；在图像识别和理解方面，它或许能提供更精准的结果；而对于语音交互，也会让人感觉更加舒适和自然。</p>\\n<h2>GPT-4o的能力</h2>\\n<p>GPT-4o 具备在音频、视觉和文本领域进行实时推理的能力。其能够接纳文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合作为输出。对于音频输入，GPT-4o 最短响应时间为 232 毫秒，平均响应时间为 320 毫秒，此响应速度与人类在对话中的表现相近。</p>","autoDesc":true}');export{y as comp,A as data};
